# Transformer in Transformer as Backbone for Deep Reinforcement Learning
Hangyu Mao†, Rui Zhao, [Zhiwei Xu](https://github.com/deligentfool), Hao Chen, Jianye Hao, Ziyue Li, Yiqun Chen, Dong Li, Bin Zhang, Zhen Xiao, Haoyuan Jiang, Junge Zhang, and Jiangjin Yin.  
†The corresponding author



## Overview
This is the official implementation of Transformer in Transformer (TIT) as Backbone for Deep Reinforcement Learning.

Paper: [old version](https://arxiv.org/pdf/2212.14538.pdf); the new version will come soon.

Code: 
1) The two folders, DT_TIT and PPO_TIT_and_CQL_TIT, contain the old version; 
2) The two folders, RL_Foundation_Mujoco_including_DT_MGDT_GATO_and_TIT and RLFoundation_BabyAI_including_DT_GATO_and_TIT, contain the new version; 
3) **We recommend the readers to use the new version, which has a satisfactory performance and good file structure (thus, is easy to modify to design new algorithms). Thanks to [Dr. Zhiwei Xu](https://github.com/deligentfool) for the contribution of this new version.**


## Cite
Please cite our paper as:
```
@article{mao2022TIT,
  title={Transformer in Transformer as Backbone for Deep Reinforcement Learning},
  author={Mao, Hangyu and Zhao, Rui and Chen, Hao and Hao, Jianye and Chen, Yiqun and Li, Dong and Zhang, Junge and Xiao, Zhen},
  journal={arXiv preprint arXiv:2212.14538},
  year={2022}
}
```



## License
MIT
